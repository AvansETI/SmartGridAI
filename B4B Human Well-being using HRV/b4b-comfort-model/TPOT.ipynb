{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier, TPOTRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              DateTime       RMSSD  User Name  Air Qualityall Good  Beverage  \\\n0  2022-12-12 13:28:19   30.287191          1                  1.0       2.0   \n1  2022-12-12 13:28:43  103.473443          0                  1.0       2.0   \n2  2022-12-12 13:31:19   25.434576          1                  1.0       2.0   \n3  2022-12-12 13:34:52   26.282239          1                  1.0       2.0   \n4  2022-12-12 13:39:23   31.830500          1                  1.0       2.0   \n\n   Cloth 1  Cloth 2  Cloth 3  Cloth 4  Cloth 5  ...  Location_LD124  \\\n0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n1      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n2      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n3      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n4      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n\n   Location_LD125  Location_LD323  Location_LD328  Humidity  Temperature  \\\n0             0.0             0.0             0.0      35.5         21.4   \n1             0.0             0.0             0.0      35.5         21.4   \n2             0.0             0.0             0.0      35.5         21.4   \n3             0.0             0.0             0.0      35.6         21.4   \n4             0.0             0.0             0.0      35.7         21.5   \n\n   TemperatureF   Light    eCO2   TVOC  \n0         70.52  1106.0   530.0   19.0  \n1         70.52  1106.0   530.0   19.0  \n2         70.52   992.0   584.0   28.0  \n3         70.52  1118.0   589.0   28.0  \n4         70.70  1115.0  2018.0  779.0  \n\n[5 rows x 48 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DateTime</th>\n      <th>RMSSD</th>\n      <th>User Name</th>\n      <th>Air Qualityall Good</th>\n      <th>Beverage</th>\n      <th>Cloth 1</th>\n      <th>Cloth 2</th>\n      <th>Cloth 3</th>\n      <th>Cloth 4</th>\n      <th>Cloth 5</th>\n      <th>...</th>\n      <th>Location_LD124</th>\n      <th>Location_LD125</th>\n      <th>Location_LD323</th>\n      <th>Location_LD328</th>\n      <th>Humidity</th>\n      <th>Temperature</th>\n      <th>TemperatureF</th>\n      <th>Light</th>\n      <th>eCO2</th>\n      <th>TVOC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-12-12 13:28:19</td>\n      <td>30.287191</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1106.0</td>\n      <td>530.0</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-12-12 13:28:43</td>\n      <td>103.473443</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1106.0</td>\n      <td>530.0</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-12-12 13:31:19</td>\n      <td>25.434576</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>992.0</td>\n      <td>584.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-12-12 13:34:52</td>\n      <td>26.282239</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.6</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1118.0</td>\n      <td>589.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-12-12 13:39:23</td>\n      <td>31.830500</td>\n      <td>1</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.7</td>\n      <td>21.5</td>\n      <td>70.70</td>\n      <td>1115.0</td>\n      <td>2018.0</td>\n      <td>779.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 48 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('final.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df_y = df['Thermal Comfort']\n",
    "df_X = df.drop(columns=['Thermal Comfort', 'DateTime', 'User Name', 'Timestamp'], axis=1)\n",
    "# Remove strings because tpot does not support them\n",
    "\n",
    "# Split the data in a train and test set with each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "train_X, text_X = train_test_split(df_X, test_size=0.3, random_state=42)\n",
    "train_y, test_y = train_test_split(df_y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\tpot\\tpot.py:67: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  self.pretest_y[0:unique_target_idx.shape[0]] = \\\n"
     ]
    },
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f32e1f5f653040a8a97065d57d588e6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.669147005444646\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6724137931034482\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6764670296430731\tRandomForestClassifier(CombineDFs(input_matrix, input_matrix), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6901391409558377\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6901391409558377\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6901391409558377\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6901391409558377\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-4\t0.6972776769509983\tDecisionTreeClassifier(SelectPercentile(StandardScaler(CombineDFs(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=1.0, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.9500000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.15000000000000002), input_matrix)), SelectPercentile__percentile=93), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-4\t0.6972776769509983\tDecisionTreeClassifier(SelectPercentile(StandardScaler(CombineDFs(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=1.0, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.9500000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.15000000000000002), input_matrix)), SelectPercentile__percentile=93), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _mate_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428916\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.45), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.704174228675136\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=6)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.704174228675136\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=6)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.704174228675136\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=6)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by BernoulliNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by FastICA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-4\t0.7111917725347852\tDecisionTreeClassifier(RFE(MinMaxScaler(SGDClassifier(input_matrix, SGDClassifier__alpha=0.0, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=False, SGDClassifier__l1_ratio=0.0, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=100.0)), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-4\t0.7111917725347852\tDecisionTreeClassifier(RFE(MinMaxScaler(SGDClassifier(input_matrix, SGDClassifier__alpha=0.0, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=False, SGDClassifier__l1_ratio=0.0, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=100.0)), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:765: FutureWarning: sklearn.metrics.SCORERS is deprecated and will be removed in v1.3. Please use sklearn.metrics.get_scorer_names to get a list of available scorers and sklearn.metrics.get_metric to get scorer.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6774193548387096"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=100, population_size=100, verbosity=3, n_jobs=-1)\n",
    "tpot.fit(train_X, train_y)\n",
    "tpot.score(text_X, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline_v1.py')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# After feature importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_y = df['Thermal Comfort']\n",
    "df_X = df[['Thermal Preference', 'TemperatureF', 'Humidity', 'Mood', 'Mode Of Transport', 'Eat Recent_Two hours ago', 'Light', 'TVOC', 'Cloth 2']]\n",
    "# Remove strings because tpot does not support them\n",
    "\n",
    "# Split the data in a train and test set with each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "train_X, text_X = train_test_split(df_X, test_size=0.3, random_state=42)\n",
    "train_y, test_y = train_test_split(df_y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\tpot\\tpot.py:67: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  self.pretest_y[0:unique_target_idx.shape[0]] = \\\n"
     ]
    },
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2823140ed7d644b087085bce694454a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6609800362976406\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6609800362976406\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6609800362976406\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6609800362976406\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6609800362976406\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6609800362976406\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6609800362976406\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.6644283121597097\tGaussianNB(SelectFromModel(RobustScaler(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6609800362976406\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.6644283121597097\tGaussianNB(SelectFromModel(RobustScaler(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6506352087114338\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6506352087114338\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6575317604355717\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6506352087114338\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6610405323653963\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.3, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6506352087114338\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6610405323653963\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.3, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6506352087114338\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6610405323653963\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.3, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.650756200846945\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6610405323653963\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.3, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6610405323653963\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.3, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6610405323653963\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.3, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6610405323653963\tGaussianNB(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.3, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=12, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6611010284331519\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6611010284331519\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=7 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6611010284331519\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6611010284331519\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6678765880217786\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6644283121597095\tExtraTreesClassifier(MinMaxScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.657471264367816\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6679370840895342\tGaussianNB(SelectFromModel(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=1.0, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575922565033273\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575922565033273\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575922565033273\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6715063520871143\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=9, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by BernoulliNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575922565033273\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6715063520871143\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=9, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.678281911675741\tGaussianNB(SelectFromModel(RandomForestClassifier(MinMaxScaler(input_matrix), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "\n",
      "-5\t0.68173018753781\tGaussianNB(SelectFromModel(RobustScaler(SelectPercentile(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100), SelectPercentile__percentile=38)), SelectFromModel__ExtraTreesClassifier__criterion=entropy, SelectFromModel__ExtraTreesClassifier__max_features=0.35000000000000003, SelectFromModel__ExtraTreesClassifier__n_estimators=100, SelectFromModel__threshold=0.1))\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575922565033273\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575922565033273\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PCA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575922565033273\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.7500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.664670296430732\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.668118572292801\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by FastICA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.668118572292801\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.6000000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=7 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=8 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Nystroem..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by BernoulliNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Normalizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by BernoulliNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PCA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6576527525710829\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PCA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Nystroem..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PCA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Nystroem..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by BernoulliNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=6 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=7 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=8 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=9 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PCA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Binarizer..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LogisticRegression..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by Nystroem..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PCA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RBFSampler..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by BernoulliNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _mate_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6678765880217785\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6784029038112522\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=8, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=11, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6819116757410769\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(input_matrix, SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=15), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.6819721718088324\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(SelectFwe(PolynomialFeatures(input_matrix, PolynomialFeatures__degree=2, PolynomialFeatures__include_bias=False, PolynomialFeatures__interaction_only=False), SelectFwe__alpha=0.022), input_matrix), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=20), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.45, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:765: FutureWarning: sklearn.metrics.SCORERS is deprecated and will be removed in v1.3. Please use sklearn.metrics.get_scorer_names to get a list of available scorers and sklearn.metrics.get_metric to get scorer.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6693548387096774"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=100, population_size=100, verbosity=3, n_jobs=-1)\n",
    "tpot.fit(train_X, train_y)\n",
    "tpot.score(text_X, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline_trimmed_v2.py')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model with RMSSD and temperature in celcius"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "df_y = df['Thermal Comfort']\n",
    "df_X = df[['Thermal Preference', 'TemperatureF', 'Humidity', 'Mood', 'Mode Of Transport', 'Eat Recent_Two hours ago', 'Light', 'TVOC', 'Cloth 2', 'RMSSD']]\n",
    "# Remove strings because tpot does not support them\n",
    "\n",
    "# Split the data in a train and test set with each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "train_X, text_X = train_test_split(df_X, test_size=0.3, random_state=42)\n",
    "train_y, test_y = train_test_split(df_y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0be87f78a37f40dc9a5807e70c943528"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6298245614035087\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6298245614035087\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8, RandomForestClassifier__min_samples_leaf=8, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6299455535390199\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6331518451300665\tRandomForestClassifier(RobustScaler(input_matrix), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=5, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6369630973986691\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6436176648517847\tRandomForestClassifier(Normalizer(RobustScaler(input_matrix), Normalizer__norm=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6369630973986691\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6402298850574712\tRandomForestClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=9, DecisionTreeClassifier__min_samples_split=19), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=3, RandomForestClassifier__min_samples_split=7, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6436176648517847\tRandomForestClassifier(Normalizer(RobustScaler(input_matrix), Normalizer__norm=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6369630973986691\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6402298850574712\tRandomForestClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=9, DecisionTreeClassifier__min_samples_split=19), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=3, RandomForestClassifier__min_samples_split=7, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6436176648517847\tRandomForestClassifier(Normalizer(RobustScaler(input_matrix), Normalizer__norm=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6369630973986691\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6402298850574712\tRandomForestClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=9, DecisionTreeClassifier__min_samples_split=19), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=3, RandomForestClassifier__min_samples_split=7, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6436176648517847\tRandomForestClassifier(Normalizer(RobustScaler(input_matrix), Normalizer__norm=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6369630973986691\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6402298850574712\tRandomForestClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=9, DecisionTreeClassifier__min_samples_split=19), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=3, RandomForestClassifier__min_samples_split=7, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6436176648517847\tRandomForestClassifier(Normalizer(RobustScaler(input_matrix), Normalizer__norm=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6369630973986691\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6402298850574712\tRandomForestClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=9, DecisionTreeClassifier__min_samples_split=19), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=3, RandomForestClassifier__min_samples_split=7, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6436176648517847\tRandomForestClassifier(Normalizer(RobustScaler(input_matrix), Normalizer__norm=l2), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.8500000000000001, RandomForestClassifier__min_samples_leaf=2, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6369630973986691\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6437386569872958\tExtraTreesClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.45), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6471869328493648\tExtraTreesClassifier(SelectPercentile(MinMaxScaler(input_matrix), SelectPercentile__percentile=99), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6369630973986691\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6437991530550514\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6437991530550514\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GaussianNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6437991530550514\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6437991530550514\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6577132486388385\tExtraTreesClassifier(Normalizer(StandardScaler(input_matrix), Normalizer__norm=l1), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6577132486388385\tExtraTreesClassifier(Normalizer(StandardScaler(input_matrix), Normalizer__norm=l1), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6577132486388385\tExtraTreesClassifier(Normalizer(StandardScaler(input_matrix), Normalizer__norm=l1), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6577132486388385\tExtraTreesClassifier(Normalizer(StandardScaler(input_matrix), Normalizer__norm=l1), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6472474289171204\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6576527525710829\tExtraTreesClassifier(RobustScaler(input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6577132486388385\tExtraTreesClassifier(Normalizer(StandardScaler(input_matrix), Normalizer__norm=l1), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6541439806412583\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6609195402298851\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6609195402298851\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6609195402298851\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6609195402298851\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6609195402298851\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6575317604355717\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6609195402298851\tExtraTreesClassifier(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6610405323653962\tExtraTreesClassifier(CombineDFs(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6610405323653962\tExtraTreesClassifier(CombineDFs(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6610405323653963\tExtraTreesClassifier(CombineDFs(CombineDFs(RobustScaler(input_matrix), input_matrix), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=4, ExtraTreesClassifier__min_samples_split=7, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6610405323653962\tExtraTreesClassifier(CombineDFs(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6645493042952209\tExtraTreesClassifier(CombineDFs(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.6000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.35000000000000003), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6610405323653962\tExtraTreesClassifier(CombineDFs(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6645493042952209\tExtraTreesClassifier(CombineDFs(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.6000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.35000000000000003), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6610405323653962\tExtraTreesClassifier(CombineDFs(CombineDFs(input_matrix, CombineDFs(input_matrix, input_matrix)), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.4, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6645493042952209\tExtraTreesClassifier(CombineDFs(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.6000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.35000000000000003), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6643678160919539\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6645493042952209\tExtraTreesClassifier(CombineDFs(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.6000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.35000000000000003), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6643678160919539\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6645493042952209\tExtraTreesClassifier(CombineDFs(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.6000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.35000000000000003), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6643678160919539\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6645493042952209\tExtraTreesClassifier(CombineDFs(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.6000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.35000000000000003), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6643678160919539\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=4, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6645493042952209\tExtraTreesClassifier(CombineDFs(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.6000000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.35000000000000003), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6679370840895342\tExtraTreesClassifier(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(CombineDFs(input_matrix, MultinomialNB(SelectFwe(input_matrix, SelectFwe__alpha=0.048), MultinomialNB__alpha=0.01, MultinomialNB__fit_prior=False)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6679370840895342\tExtraTreesClassifier(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(CombineDFs(input_matrix, MultinomialNB(SelectFwe(input_matrix, SelectFwe__alpha=0.048), MultinomialNB__alpha=0.01, MultinomialNB__fit_prior=False)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6679370840895342\tExtraTreesClassifier(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(CombineDFs(input_matrix, MultinomialNB(SelectFwe(input_matrix, SelectFwe__alpha=0.048), MultinomialNB__alpha=0.01, MultinomialNB__fit_prior=False)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6679370840895342\tExtraTreesClassifier(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(CombineDFs(input_matrix, MultinomialNB(SelectFwe(input_matrix, SelectFwe__alpha=0.048), MultinomialNB__alpha=0.01, MultinomialNB__fit_prior=False)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6679370840895342\tExtraTreesClassifier(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(CombineDFs(input_matrix, MultinomialNB(SelectFwe(input_matrix, SelectFwe__alpha=0.048), MultinomialNB__alpha=0.01, MultinomialNB__fit_prior=False)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6679370840895342\tExtraTreesClassifier(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6680580762250454\tExtraTreesClassifier(CombineDFs(input_matrix, MultinomialNB(SelectFwe(input_matrix, SelectFwe__alpha=0.048), MultinomialNB__alpha=0.01, MultinomialNB__fit_prior=False)), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6750151240169389\tExtraTreesClassifier(CombineDFs(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6750151240169389\tExtraTreesClassifier(CombineDFs(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6750151240169389\tExtraTreesClassifier(CombineDFs(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6750151240169389\tExtraTreesClassifier(CombineDFs(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6750151240169389\tExtraTreesClassifier(CombineDFs(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6750151240169389\tExtraTreesClassifier(CombineDFs(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6644888082274651\tExtraTreesClassifier(CombineDFs(input_matrix, input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=2, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6750151240169389\tExtraTreesClassifier(CombineDFs(MLPClassifier(input_matrix, MLPClassifier__alpha=0.01, MLPClassifier__learning_rate_init=0.01), input_matrix), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.9500000000000001, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=100, population_size=100, verbosity=3, n_jobs=-1)\n",
    "tpot.fit(train_X, train_y)\n",
    "tpot.score(text_X, test_y)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline_trimmed_v3.py')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Regression"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df_y = df['Thermal Comfort']\n",
    "df_X = df[['Thermal Preference', 'TemperatureF', 'Humidity', 'Mood', 'Mode Of Transport', 'Eat Recent_Two hours ago', 'Light', 'TVOC', 'Cloth 2', 'RMSSD']]\n",
    "# Remove strings because tpot does not support them\n",
    "\n",
    "# Split the data in a train and test set with each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "train_X, text_X = train_test_split(df_X, test_size=0.3, random_state=42)\n",
    "train_y, test_y = train_test_split(df_y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f7269247c954169a6838551ba31537f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3547822478378836\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9500000000000001, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.34408701278368836\tRandomForestRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=11, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=8, RandomForestRegressor__min_samples_split=13, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3547822478378836\tRandomForestRegressor(input_matrix, RandomForestRegressor__bootstrap=True, RandomForestRegressor__max_features=0.9500000000000001, RandomForestRegressor__min_samples_leaf=5, RandomForestRegressor__min_samples_split=8, RandomForestRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.34408701278368836\tRandomForestRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=11, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100), RandomForestRegressor__bootstrap=False, RandomForestRegressor__max_features=0.5, RandomForestRegressor__min_samples_leaf=8, RandomForestRegressor__min_samples_split=13, RandomForestRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.33568094864135317\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.33568094864135317\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3281164415723867\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.5, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=7, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.3247500069738523\tExtraTreesRegressor(ExtraTreesRegressor(RobustScaler(MinMaxScaler(input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=16, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.32703254402984233\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.3247500069738523\tExtraTreesRegressor(ExtraTreesRegressor(RobustScaler(MinMaxScaler(input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=16, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by AdaBoostRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X contains infinity or a value too large for dtype('float32')..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.32703254402984233\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.9000000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.3247500069738523\tExtraTreesRegressor(ExtraTreesRegressor(RobustScaler(MinMaxScaler(input_matrix)), ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=16, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.8500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LassoLarsCV..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3168812842672414\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestRegressor..\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3168812842672414\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3168812842672414\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3168812842672414\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.31411290986085905\tExtraTreesRegressor(SGDRegressor(input_matrix, SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3168812842672414\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.31411290986085905\tExtraTreesRegressor(SGDRegressor(input_matrix, SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=huber, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=2, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3168812842672414\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.31250067709383617\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=16, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3135767847919608\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.31250067709383617\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.2, ExtraTreesRegressor__min_samples_leaf=15, ExtraTreesRegressor__min_samples_split=16, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.7500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3135767847919608\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3098412613430127\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3135767847919608\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3098412613430127\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3135767847919608\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3098412613430127\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3082045765275257\tExtraTreesRegressor(SGDRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3115537814406307\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3098412613430127\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3082045765275257\tExtraTreesRegressor(SGDRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3115537814406307\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.6500000000000001, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3098412613430127\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.55, ExtraTreesRegressor__min_samples_leaf=2, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3082045765275257\tExtraTreesRegressor(SGDRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3096285647307925\tExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3082045765275257\tExtraTreesRegressor(SGDRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3096285647307925\tExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3082045765275257\tExtraTreesRegressor(SGDRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3096285647307925\tExtraTreesRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.005), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3082045765275257\tExtraTreesRegressor(SGDRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30961977309773475\tExtraTreesRegressor(SGDRegressor(input_matrix, SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3082045765275257\tExtraTreesRegressor(SGDRegressor(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), SGDRegressor__alpha=0.0, SGDRegressor__eta0=0.01, SGDRegressor__fit_intercept=True, SGDRegressor__l1_ratio=0.75, SGDRegressor__learning_rate=constant, SGDRegressor__loss=epsilon_insensitive, SGDRegressor__penalty=elasticnet, SGDRegressor__power_t=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.309159004461585\tExtraTreesRegressor(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3064873032197352\tExtraTreesRegressor(CombineDFs(input_matrix, Nystroem(input_matrix, Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=4)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3064873032197352\tExtraTreesRegressor(CombineDFs(input_matrix, Nystroem(input_matrix, Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=4)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3064873032197352\tExtraTreesRegressor(CombineDFs(input_matrix, Nystroem(input_matrix, Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=4)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3064873032197352\tExtraTreesRegressor(CombineDFs(input_matrix, Nystroem(input_matrix, Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=4)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3064873032197352\tExtraTreesRegressor(CombineDFs(input_matrix, Nystroem(input_matrix, Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=4)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3064873032197352\tExtraTreesRegressor(CombineDFs(input_matrix, Nystroem(input_matrix, Nystroem__gamma=0.15000000000000002, Nystroem__kernel=laplacian, Nystroem__n_components=4)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30429607985480944\tExtraTreesRegressor(StandardScaler(input_matrix), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3032792816091954\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.30324791788499034\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.29839368874773137\tExtraTreesRegressor(VarianceThreshold(RobustScaler(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.29839368874773137\tExtraTreesRegressor(VarianceThreshold(RobustScaler(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.29839368874773137\tExtraTreesRegressor(VarianceThreshold(RobustScaler(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-7\t-0.2980599621899577\tExtraTreesRegressor(RobustScaler(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(MinMaxScaler(input_matrix))), VarianceThreshold__threshold=0.01))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.29839368874773137\tExtraTreesRegressor(VarianceThreshold(RobustScaler(VarianceThreshold(MinMaxScaler(input_matrix), VarianceThreshold__threshold=0.01)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-7\t-0.2980599621899577\tExtraTreesRegressor(RobustScaler(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(MinMaxScaler(input_matrix))), VarianceThreshold__threshold=0.01))), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.30152243146971835\tExtraTreesRegressor(SelectFwe(LinearSVR(ExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=True, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=10, ExtraTreesRegressor__min_samples_split=6, ExtraTreesRegressor__n_estimators=100), LinearSVR__C=0.01, LinearSVR__dual=True, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.001), SelectFwe__alpha=0.017), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=5, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.2927657032667876\tExtraTreesRegressor(MaxAbsScaler(VarianceThreshold(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.2927657032667876\tExtraTreesRegressor(MaxAbsScaler(VarianceThreshold(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.2927657032667876\tExtraTreesRegressor(MaxAbsScaler(VarianceThreshold(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.2927657032667876\tExtraTreesRegressor(MaxAbsScaler(VarianceThreshold(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVR..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _mate_operator: num_test=0 No feature in X meets the variance threshold 0.01000.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.2927657032667876\tExtraTreesRegressor(MaxAbsScaler(VarianceThreshold(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.2927657032667876\tExtraTreesRegressor(MaxAbsScaler(VarianceThreshold(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 90.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.3012966364186328\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.45, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.2990327132486389\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2961488112522686\tExtraTreesRegressor(RobustScaler(VarianceThreshold(MinMaxScaler(MinMaxScaler(input_matrix)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3024883424077435\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29672547758284606\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29563386437798206\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29563386437798206\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29563386437798206\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Automatic alpha grid generation is not supported for l1_ratio=0. Please supply a grid by providing your estimator with the appropriate `alphas=` argument..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesRegressor..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='epsilon_insensitive' are not supported when dual=False, Parameters: penalty='l2', loss='epsilon_insensitive', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t-0.3097465214761041\tExtraTreesRegressor(input_matrix, ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.25, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-2\t-0.3015187887342879\tExtraTreesRegressor(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.001, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-3\t-0.2976572247428917\tExtraTreesRegressor(SelectFromModel(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001), SelectFromModel__ExtraTreesRegressor__max_features=0.6000000000000001, SelectFromModel__ExtraTreesRegressor__n_estimators=100, SelectFromModel__threshold=0.0), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.3, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-4\t-0.29563386437798206\tExtraTreesRegressor(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.35000000000000003, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-5\t-0.2953777505545474\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001)), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=4, ExtraTreesRegressor__n_estimators=100)\n",
      "\n",
      "-6\t-0.29205888082274656\tExtraTreesRegressor(MinMaxScaler(VarianceThreshold(MinMaxScaler(StandardScaler(LinearSVR(input_matrix, LinearSVR__C=0.01, LinearSVR__dual=False, LinearSVR__epsilon=0.1, LinearSVR__loss=squared_epsilon_insensitive, LinearSVR__tol=0.0001))), VarianceThreshold__threshold=0.01)), ExtraTreesRegressor__bootstrap=False, ExtraTreesRegressor__max_features=0.4, ExtraTreesRegressor__min_samples_leaf=1, ExtraTreesRegressor__min_samples_split=3, ExtraTreesRegressor__n_estimators=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:765: FutureWarning: sklearn.metrics.SCORERS is deprecated and will be removed in v1.3. Please use sklearn.metrics.get_scorer_names to get a list of available scorers and sklearn.metrics.get_metric to get scorer.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but LinearSVR was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "-0.4549219758064516"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=100, population_size=100, verbosity=3, n_jobs=-1)\n",
    "tpot.fit(train_X, train_y)\n",
    "tpot.score(text_X, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline_trimmed_regression.py')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
