{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              DateTime       RMSSD       User Name  Air Qualityall Good  \\\n0  2022-12-12 13:28:19   30.287191  Noah@email.com                  1.0   \n1  2022-12-12 13:28:43  103.473443   job@email.com                  1.0   \n2  2022-12-12 13:31:19   25.434576  Noah@email.com                  1.0   \n3  2022-12-12 13:34:52   26.282239  Noah@email.com                  1.0   \n4  2022-12-12 13:39:23   31.830500  Noah@email.com                  1.0   \n\n   Beverage  Cloth 1  Cloth 2  Cloth 3  Cloth 4  Cloth 5  ...  Location_LD106  \\\n0       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n1       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n2       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n3       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n4       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n\n   Location_LD124  Location_LD323  Location_LD328  Humidity  Temperature  \\\n0             0.0             0.0             0.0      35.5         21.4   \n1             0.0             0.0             0.0      35.5         21.4   \n2             0.0             0.0             0.0      35.5         21.4   \n3             0.0             0.0             0.0      35.6         21.4   \n4             0.0             0.0             0.0      35.7         21.5   \n\n   TemperatureF   Light    eCO2   TVOC  \n0         70.52  1106.0   530.0   19.0  \n1         70.52  1106.0   530.0   19.0  \n2         70.52   992.0   584.0   28.0  \n3         70.52  1118.0   589.0   28.0  \n4         70.70  1115.0  2018.0  779.0  \n\n[5 rows x 46 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DateTime</th>\n      <th>RMSSD</th>\n      <th>User Name</th>\n      <th>Air Qualityall Good</th>\n      <th>Beverage</th>\n      <th>Cloth 1</th>\n      <th>Cloth 2</th>\n      <th>Cloth 3</th>\n      <th>Cloth 4</th>\n      <th>Cloth 5</th>\n      <th>...</th>\n      <th>Location_LD106</th>\n      <th>Location_LD124</th>\n      <th>Location_LD323</th>\n      <th>Location_LD328</th>\n      <th>Humidity</th>\n      <th>Temperature</th>\n      <th>TemperatureF</th>\n      <th>Light</th>\n      <th>eCO2</th>\n      <th>TVOC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-12-12 13:28:19</td>\n      <td>30.287191</td>\n      <td>Noah@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1106.0</td>\n      <td>530.0</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-12-12 13:28:43</td>\n      <td>103.473443</td>\n      <td>job@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1106.0</td>\n      <td>530.0</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-12-12 13:31:19</td>\n      <td>25.434576</td>\n      <td>Noah@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>992.0</td>\n      <td>584.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-12-12 13:34:52</td>\n      <td>26.282239</td>\n      <td>Noah@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.6</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1118.0</td>\n      <td>589.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-12-12 13:39:23</td>\n      <td>31.830500</td>\n      <td>Noah@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.7</td>\n      <td>21.5</td>\n      <td>70.70</td>\n      <td>1115.0</td>\n      <td>2018.0</td>\n      <td>779.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 46 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('final.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df_y = df['Thermal Comfort']\n",
    "df_X = df.drop(columns=['Thermal Comfort', 'DateTime', 'User Name', 'Timestamp'], axis=1)\n",
    "# Remove strings because tpot does not support them\n",
    "\n",
    "# Split the data in a train and test set with each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "train_X, text_X = train_test_split(df_X, test_size=0.3, random_state=42)\n",
    "train_y, test_y = train_test_split(df_y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefan\\anaconda3\\envs\\tpot\\lib\\site-packages\\tpot\\tpot.py:67: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  self.pretest_y[0:unique_target_idx.shape[0]] = \\\n"
     ]
    },
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "93bcf9d77e3342deb9e434e10545d3f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6416666666666667\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7500000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6541666666666667\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7500000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6541666666666667\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7500000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6541666666666667\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7500000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6541666666666667\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7500000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=19, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6583333333333333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6583333333333333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6833333333333333\tExtraTreesClassifier(VarianceThreshold(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), VarianceThreshold__threshold=0.0001), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6583333333333333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6583333333333333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6708333333333333\tRandomForestClassifier(RandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.8, RandomForestClassifier__min_samples_leaf=19, RandomForestClassifier__min_samples_split=11, RandomForestClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.7500000000000001, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=4, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6583333333333333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6583333333333333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6583333333333333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6583333333333333\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.55, RandomForestClassifier__min_samples_leaf=10, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6625\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6625\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6625\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6625\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6625\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6625\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=7, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.\n",
      "Invalid pipeline encountered. Skipping its evaluation.\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6666666666666666\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6666666666666666\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7000000000000001\tExtraTreesClassifier(MLPClassifier(RFE(VarianceThreshold(VarianceThreshold(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), VarianceThreshold__threshold=0.0001), VarianceThreshold__threshold=0.0001), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.15000000000000002, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), MLPClassifier__alpha=0.1, MLPClassifier__learning_rate_init=0.1), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6666666666666666\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.6958333333333333\tExtraTreesClassifier(SelectPercentile(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectPercentile__percentile=76), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6666666666666666\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6666666666666666\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6666666666666666\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6666666666666666\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6666666666666666\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.7000000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6874999999999999\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6916666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7\tExtraTreesClassifier(CombineDFs(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=20), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=9), input_matrix), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7125\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7125\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7125\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7125\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7125\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7083333333333334\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), SelectFwe__alpha=0.024), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7125\tExtraTreesClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=2, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=12), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7000000000000001\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7041666666666666\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=8), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=3, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7041666666666667\tExtraTreesClassifier(StandardScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.00010.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7083333333333333\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7083333333333333\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(DecisionTreeClassifier(RFE(CombineDFs(input_matrix, input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.9500000000000001, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=16), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7083333333333333\tExtraTreesClassifier(DecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7166666666666667\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=9), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 99.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7166666666666668\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(MaxAbsScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7166666666666668\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(MaxAbsScaler(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2)), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333333\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333333\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00010.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-5\t0.7250000000000001\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(SGDClassifier(CombineDFs(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), input_matrix), SGDClassifier__alpha=0.01, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=True, SGDClassifier__l1_ratio=0.5, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=50.0), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7291666666666667\tExtraTreesClassifier(MaxAbsScaler(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.7208333333333334\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7291666666666667\tExtraTreesClassifier(MaxAbsScaler(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.725\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=20), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7291666666666667\tExtraTreesClassifier(MaxAbsScaler(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.725\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=20), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7291666666666667\tExtraTreesClassifier(MaxAbsScaler(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.725\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=20), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7291666666666667\tExtraTreesClassifier(MaxAbsScaler(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.675\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.6500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=18, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.7166666666666667\tExtraTreesClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=5), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.3, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-3\t0.725\tExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=20), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-4\t0.7291666666666667\tExtraTreesClassifier(MaxAbsScaler(SelectFwe(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=14, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.023)), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=1.0, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=3, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "-6\t0.7333333333333332\tExtraTreesClassifier(RandomForestClassifier(ExtraTreesClassifier(SelectFwe(DecisionTreeClassifier(DecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=4, DecisionTreeClassifier__min_samples_leaf=12, DecisionTreeClassifier__min_samples_split=2), DecisionTreeClassifier__criterion=gini, DecisionTreeClassifier__max_depth=3, DecisionTreeClassifier__min_samples_leaf=19, DecisionTreeClassifier__min_samples_split=13), SelectFwe__alpha=0.03), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=5, ExtraTreesClassifier__min_samples_split=13, ExtraTreesClassifier__n_estimators=100), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.2, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=3, RandomForestClassifier__n_estimators=100), ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.8500000000000001, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=6, ExtraTreesClassifier__n_estimators=100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefan\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:765: FutureWarning: sklearn.metrics.SCORERS is deprecated and will be removed in v1.3. Please use sklearn.metrics.get_scorer_names to get a list of available scorers and sklearn.metrics.get_metric to get scorer.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Stefan\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\Stefan\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6153846153846154"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=100, population_size=100, verbosity=3, n_jobs=-1)\n",
    "tpot.fit(train_X, train_y)\n",
    "tpot.score(text_X, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
