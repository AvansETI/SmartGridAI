{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "              DateTime       RMSSD       User Name  Air Qualityall Good  \\\n0  2022-12-12 13:28:19   30.287191  Noah@email.com                  1.0   \n1  2022-12-12 13:28:43  103.473443   job@email.com                  1.0   \n2  2022-12-12 13:31:19   25.434576  Noah@email.com                  1.0   \n3  2022-12-12 13:34:52   26.282239  Noah@email.com                  1.0   \n4  2022-12-12 13:39:23   31.830500  Noah@email.com                  1.0   \n\n   Beverage  Cloth 1  Cloth 2  Cloth 3  Cloth 4  Cloth 5  ...  Location_LD124  \\\n0       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n1       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n2       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n3       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n4       2.0      0.0      0.0      1.0      0.0      0.0  ...             0.0   \n\n   Location_LD125  Location_LD323  Location_LD328  Humidity  Temperature  \\\n0             0.0             0.0             0.0      35.5         21.4   \n1             0.0             0.0             0.0      35.5         21.4   \n2             0.0             0.0             0.0      35.5         21.4   \n3             0.0             0.0             0.0      35.6         21.4   \n4             0.0             0.0             0.0      35.7         21.5   \n\n   TemperatureF   Light    eCO2   TVOC  \n0         70.52  1106.0   530.0   19.0  \n1         70.52  1106.0   530.0   19.0  \n2         70.52   992.0   584.0   28.0  \n3         70.52  1118.0   589.0   28.0  \n4         70.70  1115.0  2018.0  779.0  \n\n[5 rows x 48 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DateTime</th>\n      <th>RMSSD</th>\n      <th>User Name</th>\n      <th>Air Qualityall Good</th>\n      <th>Beverage</th>\n      <th>Cloth 1</th>\n      <th>Cloth 2</th>\n      <th>Cloth 3</th>\n      <th>Cloth 4</th>\n      <th>Cloth 5</th>\n      <th>...</th>\n      <th>Location_LD124</th>\n      <th>Location_LD125</th>\n      <th>Location_LD323</th>\n      <th>Location_LD328</th>\n      <th>Humidity</th>\n      <th>Temperature</th>\n      <th>TemperatureF</th>\n      <th>Light</th>\n      <th>eCO2</th>\n      <th>TVOC</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-12-12 13:28:19</td>\n      <td>30.287191</td>\n      <td>Noah@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1106.0</td>\n      <td>530.0</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-12-12 13:28:43</td>\n      <td>103.473443</td>\n      <td>job@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1106.0</td>\n      <td>530.0</td>\n      <td>19.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-12-12 13:31:19</td>\n      <td>25.434576</td>\n      <td>Noah@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.5</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>992.0</td>\n      <td>584.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-12-12 13:34:52</td>\n      <td>26.282239</td>\n      <td>Noah@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.6</td>\n      <td>21.4</td>\n      <td>70.52</td>\n      <td>1118.0</td>\n      <td>589.0</td>\n      <td>28.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-12-12 13:39:23</td>\n      <td>31.830500</td>\n      <td>Noah@email.com</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>35.7</td>\n      <td>21.5</td>\n      <td>70.70</td>\n      <td>1115.0</td>\n      <td>2018.0</td>\n      <td>779.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 48 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('final.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "df_y = df['Thermal Comfort']\n",
    "df_X = df.drop(columns=['Thermal Comfort', 'DateTime', 'User Name', 'Timestamp'], axis=1)\n",
    "# Remove strings because tpot does not support them\n",
    "\n",
    "# Split the data in a train and test set with each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "train_X, text_X = train_test_split(df_X, test_size=0.3, random_state=42)\n",
    "train_y, test_y = train_test_split(df_y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 operators have been imported by TPOT.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\tpot\\tpot.py:67: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  self.pretest_y[0:unique_target_idx.shape[0]] = \\\n"
     ]
    },
    {
     "data": {
      "text/plain": "Optimization Progress:   0%|          | 0/10100 [00:00<?, ?pipeline/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f32e1f5f653040a8a97065d57d588e6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MultinomialNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 1 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 2 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 3 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 4 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 5 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6657592256503326\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.4, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 6 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.669147005444646\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 7 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6724137931034482\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=5, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-2\t0.6762250453720509\tDecisionTreeClassifier(ZeroCount(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 8 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6764670296430731\tRandomForestClassifier(CombineDFs(input_matrix, input_matrix), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.9500000000000001, RandomForestClassifier__min_samples_leaf=5, RandomForestClassifier__min_samples_split=2, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 9 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 10 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6901391409558377\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 11 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6901391409558377\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by GradientBoostingClassifier..\n",
      "\n",
      "Generation 12 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6901391409558377\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 13 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6797338173018754\tRandomForestClassifier(input_matrix, RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.35000000000000003, RandomForestClassifier__min_samples_leaf=1, RandomForestClassifier__min_samples_split=17, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "-2\t0.6901391409558377\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 14 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 15 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 16 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 17 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-4\t0.6972776769509983\tDecisionTreeClassifier(SelectPercentile(StandardScaler(CombineDFs(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=1.0, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.9500000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.15000000000000002), input_matrix)), SelectPercentile__percentile=93), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "\n",
      "Generation 18 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-4\t0.6972776769509983\tDecisionTreeClassifier(SelectPercentile(StandardScaler(CombineDFs(GradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=1.0, GradientBoostingClassifier__max_depth=9, GradientBoostingClassifier__max_features=0.9500000000000001, GradientBoostingClassifier__min_samples_leaf=3, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.15000000000000002), input_matrix)), SelectPercentile__percentile=93), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 19 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "\n",
      "Generation 20 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by KNeighborsClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _mate_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 21 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.6936479128856623\tDecisionTreeClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.4, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=6, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "\n",
      "Generation 22 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 71.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 23 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 24 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 25 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.6902601330913491\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 26 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 27 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 28 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58.\n",
      "\n",
      "Generation 29 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "\n",
      "Generation 30 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 31 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 32 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 65.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 33 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 34 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RandomForestClassifier..\n",
      "\n",
      "Generation 35 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 36 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "\n",
      "Generation 37 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.704174228675136\tDecisionTreeClassifier(MinMaxScaler(MaxAbsScaler(input_matrix)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7042347247428916\tDecisionTreeClassifier(MinMaxScaler(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 38 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428916\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.45), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 39 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 40 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 41 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 42 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "\n",
      "Generation 43 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 44 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 45 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "\n",
      "Generation 46 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 47 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 48 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 49 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.693708408953418\tDecisionTreeClassifier(CombineDFs(input_matrix, input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=4)\n",
      "\n",
      "-2\t0.7007259528130672\tDecisionTreeClassifier(StandardScaler(input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 50 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "\n",
      "Generation 51 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 52 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7042347247428917\tDecisionTreeClassifier(RFE(ZeroCount(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-4\t0.7077434966727163\tDecisionTreeClassifier(MinMaxScaler(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=gini, RFE__ExtraTreesClassifier__max_features=0.25, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6500000000000001), GradientBoostingClassifier__learning_rate=0.5, GradientBoostingClassifier__max_depth=3, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=7, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.4)), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=2)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 53 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 54 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7007259528130672\tDecisionTreeClassifier(CombineDFs(CombineDFs(input_matrix, input_matrix), input_matrix), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=7)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 55 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.704174228675136\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=6)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 56 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.704174228675136\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=6)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 57 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.704174228675136\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=6)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=4 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=5 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 58 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 54.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "\n",
      "Generation 59 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 60 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "\n",
      "Generation 61 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 62 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by BernoulliNB..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 63 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 64 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by RobustScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 65 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectPercentile..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "\n",
      "Generation 66 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 67 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 68 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 69 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.10000.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 70 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 71 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "\n",
      "Generation 72 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 73 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _mate_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "\n",
      "Generation 74 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by LinearSVC..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 75 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by ExtraTreesClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SelectFwe..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 76 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 84.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MLPClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 77 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by VarianceThreshold..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "\n",
      "Generation 78 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 79 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "\n",
      "Generation 80 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by PolynomialFeatures..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 81 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "\n",
      "Generation 82 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 83 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 84 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 85 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 86 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by FastICA..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 87 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 88 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by SGDClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 89 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 96.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "\n",
      "Generation 90 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 91 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 92 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 93 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "\n",
      "Generation 94 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "\n",
      "Generation 95 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "\n",
      "Generation 96 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 63.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69.\n",
      "\n",
      "Generation 97 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler..\n",
      "\n",
      "Generation 98 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "\n",
      "Generation 99 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-4\t0.7111917725347852\tDecisionTreeClassifier(RFE(MinMaxScaler(SGDClassifier(input_matrix, SGDClassifier__alpha=0.0, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=False, SGDClassifier__l1_ratio=0.0, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=100.0)), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by DecisionTreeClassifier..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 (slice(None, None, None), 0).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by RFE..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Cosine affinity cannot be used when X contains zero vectors.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Solver lbfgs supports only dual=False, got dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X needs to contain only non-negative integers..\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Negative values in data passed to MultinomialNB (input X).\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Invalid classes inferred from unique values of `y`.  Expected: [0 1 2 3 4 5], got [1. 2. 3. 4. 5. 6.].\n",
      "\n",
      "Generation 100 - Current Pareto front scores:\n",
      "\n",
      "-1\t0.7042347247428917\tDecisionTreeClassifier(input_matrix, DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=1, DecisionTreeClassifier__min_samples_split=9)\n",
      "\n",
      "-3\t0.7077434966727163\tDecisionTreeClassifier(RFE(MinMaxScaler(input_matrix), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.1), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n",
      "\n",
      "-4\t0.7111917725347852\tDecisionTreeClassifier(RFE(MinMaxScaler(SGDClassifier(input_matrix, SGDClassifier__alpha=0.0, SGDClassifier__eta0=0.1, SGDClassifier__fit_intercept=False, SGDClassifier__l1_ratio=0.0, SGDClassifier__learning_rate=invscaling, SGDClassifier__loss=modified_huber, SGDClassifier__penalty=elasticnet, SGDClassifier__power_t=100.0)), RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.05, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.6000000000000001), DecisionTreeClassifier__criterion=entropy, DecisionTreeClassifier__max_depth=5, DecisionTreeClassifier__min_samples_leaf=3, DecisionTreeClassifier__min_samples_split=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:765: FutureWarning: sklearn.metrics.SCORERS is deprecated and will be removed in v1.3. Please use sklearn.metrics.get_scorer_names to get a list of available scorers and sklearn.metrics.get_metric to get scorer.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\jobha\\anaconda3\\envs\\tpot\\lib\\site-packages\\sklearn\\base.py:450: UserWarning: X does not have valid feature names, but SGDClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.6774193548387096"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpot = TPOTClassifier(generations=100, population_size=100, verbosity=3, n_jobs=-1)\n",
    "tpot.fit(train_X, train_y)\n",
    "tpot.score(text_X, test_y)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "tpot.export('tpot_exported_pipeline_v1.py')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
